{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1a1642-4f20-44c1-9a44-35a1b9858334",
   "metadata": {},
   "source": [
    "# Big data and cartography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8baf37e-df20-48e4-b9c5-ada89ce81560",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to use Python to explore and visualize some big geospatial data. We are going to work with the following datasets:\n",
    "\n",
    "1. Point dataset of [Flickr](https://www.flickr.com/) posts in Finland 2019â€“2023 acquired through the platform's API. The posts have been stripped of all identifiable information and the exact locations obfuscated using the Laplace noise algorithm implemented in [GeoPriv QGIS plugin](https://diuke.github.io/GeoPrivPlugin/). Only attribute information remaining are pseudo-ids and month-year timestamps. (161544 datapoints)\n",
    "2. Origin-destination line dataset of student mobilities to Germany. \n",
    "3. network speeds in the first quarter of 2023 fetched from [Ookla Speedtest](https://www.speedtest.net).\n",
    "\n",
    "In addition to the Python libraries we have already worked with, we are going to get familiar with some additional python libraries that can be handy for handling and visualization of big geospatial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cbd97-358b-4c38-8ea1-3fce222fce90",
   "metadata": {},
   "source": [
    "## Flickr data from Finland\n",
    "\n",
    "As always, let's start by exploring our data a little bit. Let's load our data, make a quick plot, count the number of points, and print the head of our data first."
   ]
  },
  {
   "cell_type": "code",
   "id": "67e8c1d3-c637-4443-a706-2cdfddd4c2d2",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the data\n",
    "file_path = 'data/flickr_finland_2019_2023_obfuscated.gpkg'\n",
    "flickr_data = gpd.read_file(file_path)\n",
    "\n",
    "flickr_data.plot(markersize=2, edgecolor='none')\n",
    "print(flickr_data.head())\n",
    "print(f\"\\nThere are {len(flickr_data)} points in our dataset\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "11c9c4b0-deee-41a4-963b-2aaac9f3ae22",
   "metadata": {},
   "source": [
    "As we can see, our data is from Finland and we have 161544 points altogether representing the location of the photos. In our data, we have a column named `month_year` which as name suggests, containes the month and year when the photo was taken. Let's explore the temporal distribution of our data quickly using a time series plot. "
   ]
  },
  {
   "cell_type": "code",
   "id": "a7ac1cbd-07ad-46d4-9d52-2c1ebbad008e",
   "metadata": {},
   "source": [
    "# Convert 'month_year' to datetime format for better handling\n",
    "flickr_data['month_year'] = pd.to_datetime(flickr_data['month_year'])\n",
    "\n",
    "# Group by 'month_year' and count occurrences\n",
    "time_series = flickr_data.groupby('month_year').size()\n",
    "\n",
    "# Plotting the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_series, marker='o', linestyle='-', color='b')\n",
    "plt.title('Number of Photos per Month (2019-2023)')\n",
    "plt.xlabel('Month and Year')\n",
    "plt.ylabel('Number of Photos')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjusts plot to ensure everything fits without overlap\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f975956-39bb-4392-bb9c-2338c0045d3b",
   "metadata": {},
   "source": [
    "### Hexbin map\n",
    "\n",
    "Hexagons are increasingly favored in the visualization of large geospatial datasets for several reasons:\n",
    "\n",
    "1. **Efficient Coverage and Less Wastage:** Hexagons cover a surface area more efficiently than squares or triangles. When visualizing large areas, hexagonal tiling reduces gaps and overlaps, ensuring more uniform data representation. \n",
    "2. **Uniformity of Data Representation:** Each hexagon has six sides of equal length, which results in a more uniform distance between the center of each hexagon and any point on its boundary. This uniformity ensures that each data point within a hexagon is equally representative of its center. In contrast, square grids have varying distances from the center to the middle of the edges versus the corners, potentially introducing bias in how data is represented spatially.\n",
    "3. **Better Nearest Neighbor Analysis:** Hexagons have a unique advantage due to their six equidistant neighbors, which is beneficial for nearest neighbor analysis. This property ensures that each cell interacts more symmetrically with its surroundings, providing a more natural flow of data and smoother transitions across the grid. Squares, on the other hand, connect to their neighbors at four sides and interact less directly with their diagonal neighbors, which can complicate analyses involving spatial relationships.\n",
    "4. **Visual Appeal and Reduced Visual Errors:** Hexagonal grids tend to be more visually appealing and easier for the human eye to follow. This can enhance the overall readability and interpretation of maps. Furthermore, the equidistant properties of hexagons can reduce visual distortions that often occur with square grids, where the clustering of data points might appear more intense at the corners compared to the center.\n",
    "5. **Efficient Computation:** Despite a seemingly complex shape, algorithms for processing hexagonal grids are often more straightforward and computationally efficient for spatial data analyses than those for squares. This is due to the consistent distances and connectivity, which simplify the computation of spatial relationships and aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ac6ade4-72c8-40fa-8f51-c0e813c689a4",
   "metadata": {},
   "source": [
    "import h3\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import mapclassify\n",
    "from shapely import Polygon\n",
    "\n",
    "# for zoomin to Helsinki area, if needed\n",
    "helsinki_bounds = {\n",
    "    \"min_lon\": 24.50,\n",
    "    \"max_lon\": 25.50,\n",
    "    \"min_lat\": 60.00,\n",
    "    \"max_lat\": 60.50\n",
    "}\n",
    "\n",
    "# Function to assign hexagon using H3\n",
    "def assign_hexagon(row, resolution=8):\n",
    "    return h3.geo_to_h3(row.geometry.y, row.geometry.x, resolution)\n",
    "\n",
    "# Apply function to data\n",
    "flickr_data['hex_id'] = flickr_data.apply(assign_hexagon, axis=1)\n",
    "\n",
    "# Aggregate data within each hexagon\n",
    "hex_counts = flickr_data['hex_id'].value_counts().reset_index()\n",
    "hex_counts.columns = ['hex_id', 'count']\n",
    "\n",
    "# Generate hexagon geometries\n",
    "hex_counts['geometry'] = hex_counts['hex_id'].apply(lambda x: h3.h3_to_geo_boundary(x, geo_json=True))\n",
    "\n",
    "# Convert hexagon geometries to GeoDataFrame\n",
    "def hex_to_polygon(hex):\n",
    "    return Polygon([(lon, lat) for lon, lat in hex])\n",
    "\n",
    "hex_counts['geometry'] = hex_counts['geometry'].apply(hex_to_polygon)\n",
    "hex_gdf = gpd.GeoDataFrame(hex_counts, geometry='geometry')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Set the bounds for the plot to zoom into Helsinki. if desired \n",
    "#ax.set_xlim(helsinki_bounds[\"min_lon\"], helsinki_bounds[\"max_lon\"])\n",
    "#ax.set_ylim(helsinki_bounds[\"min_lat\"], helsinki_bounds[\"max_lat\"])\n",
    "\n",
    "hex_gdf.plot(ax=ax, column=\"count\",scheme=\"Natural_Breaks\", k=5, cmap=\"RdYlBu\", legend=True, legend_kwds={'loc': 'lower right'})\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5b5a0cd5-bebc-4bc2-b5a4-c0544d8b571e",
   "metadata": {},
   "source": [
    "Here is a breakdown of what we are doing in the block of code above: \n",
    "\n",
    "*Define a Function to Assign Hexagons:*\n",
    "```python\n",
    "def assign_hexagon(row, resolution=8):\n",
    "    return h3.geo_to_h3(row.geometry.y, row.geometry.x, resolution)\n",
    "```\n",
    "- Purpose: This function converts geographic coordinates into H3 indices. Each H3 index represents a hexagonal cell on the globe at a specified resolution.\n",
    "- Parameters:\n",
    "  - `row`: A row of a DataFrame, expected to have `geometry` attributes (`y` for latitude and `x` for longitude).\n",
    "  - `resolution`: The granularity of the hexagonal grid. Higher values create smaller hexagons.\n",
    "- Functionality: The function `h3.geo_to_h3()` takes latitude (`row.geometry.y`), longitude (`row.geometry.x`), and `resolution` to generate a unique H3 index (hexagon ID) for that location.\n",
    "\n",
    "*Apply Function to Data:*\n",
    "```python\n",
    "flickr_data['hex_id'] = flickr_data.apply(assign_hexagon, axis=1)\n",
    "```\n",
    "- Purpose: To assign an H3 hexagon ID to each record in the `flickr_data` GeoDataFrame.\n",
    "- Method: `DataFrame.apply()` applies the `assign_hexagon` function along the DataFrame's rows (`axis=1`).\n",
    "\n",
    "*Aggregate Data Within Each Hexagon:*\n",
    "```python\n",
    "hex_counts = flickr_data['hex_id'].value_counts().reset_index()\n",
    "hex_counts.columns = ['hex_id', 'count']\n",
    "```\n",
    "- Purpose: To count occurrences of each unique H3 index in `flickr_data`.\n",
    "- Functionality:\n",
    "  - `value_counts()`: This function counts how many times each unique value appears in the `hex_id` column.\n",
    "  - `reset_index()`: Converts the Series returned by `value_counts()` into a DataFrame.\n",
    "  - Renaming Columns: Columns are named to `hex_id` and `count` for clarity.\n",
    "\n",
    "*Generate Hexagon Geometries:*\n",
    "```python\n",
    "hex_counts['geometry'] = hex_counts['hex_id'].apply(lambda x: h3.h3_to_geo_boundary(x, geo_json=True))\n",
    "```\n",
    "- Purpose: To convert each H3 index back into a polygon that represents the hexagonal cell on a map.\n",
    "- Method: The `h3.h3_to_geo_boundary()` function retrieves the vertices of the hexagon associated with each H3 index, formatted for use in GeoJSON.\n",
    "\n",
    "*Convert Hexagon Geometries to GeoDataFrame:*\n",
    "```python\n",
    "def hex_to_polygon(hex):\n",
    "    return Polygon([(lon, lat) for lon, lat in hex])\n",
    "\n",
    "hex_counts['geometry'] = hex_counts['geometry'].apply(hex_to_polygon)\n",
    "hex_gdf = gpd.GeoDataFrame(hex_counts, geometry='geometry')\n",
    "```\n",
    "- Purpose: To transform the list of hexagon vertices into `shapely.Polygon` objects suitable for spatial operations and visualization.\n",
    "- Functionality:\n",
    "  - `hex_to_polygon`: A function that converts a list of (longitude, latitude) tuples into a `Polygon`.\n",
    "  - Applying the conversion function to each hexagonâ€™s vertices.\n",
    "  - Creating a `GeoDataFrame`\n",
    "\n",
    "\n",
    "We will learn how to make interactive plots later in this course. But let's have a quick exploration of our data using plotly:"
   ]
  },
  {
   "cell_type": "code",
   "id": "0a9ff128-acf4-4d07-8426-6344a841644a",
   "metadata": {},
   "source": [
    "import plotly.express as px\n",
    "import mapclassify as mc\n",
    "\n",
    "classifier = mc.NaturalBreaks(hex_gdf['count'], k=10)\n",
    "hex_gdf['count_class'] = classifier.yb  # Assign class labels to the hex_gdf\n",
    "\n",
    "# Plot with Plotly\n",
    "fig = px.choropleth_mapbox(data_frame=hex_gdf, geojson=hex_gdf.__geo_interface__,\n",
    "                           locations=\"hex_id\", color='count_class',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           featureidkey=\"properties.hex_id\",\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=5, center={\"lat\": 64.5, \"lon\": 26.0},\n",
    "                           opacity=0.5, labels={'count':'Data Points'},\n",
    "                           hover_data={'count': True}\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8e163310-39af-4727-8ace-8cf5be28653c",
   "metadata": {},
   "source": [
    "## Mapping Speedtest data with lonboard\n",
    "\n",
    "The dataset we are going to use now is from Ookla, known for its [Speedtest](https://www.speedtest.net/) application, which provides data on internet connection speeds across different regions and network types. This specific dataset that we will use here, contains mobile internet performance for the first quarter of 2023. It is stored in the Parquet format, which is highly efficient for handling large datasets as it supports advanced data compression and encoding schemes, making it suitable for big data applications.\n",
    "\n",
    "While visualizing our data, we will explore the use of the [`lonboard` library](https://developmentseed.org/lonboard/latest/). This is a new powerful library, designed for efficient handling and analysis of geospatial data. This library is particularly optimized for performance and scalability, which makes it a good choice for processing big geospatial data. \n",
    "\n",
    "\n",
    "Let's start by importing the libraries we need and fetch our data."
   ]
  },
  {
   "cell_type": "code",
   "id": "f1cb92e2-07a3-41d5-b2e5-06f349795c8a",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from palettable.colorbrewer.diverging import BrBG_10\n",
    "from lonboard import Map, ScatterplotLayer\n",
    "from lonboard.colormap import apply_continuous_cmap"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c6bb953-471e-4cb1-beff-61cf4dc98872",
   "metadata": {},
   "source": [
    "# Fetching data\n",
    "url = \"https://ookla-open-data.s3.us-west-2.amazonaws.com/parquet/performance/type=mobile/year=2023/quarter=1/2023-01-01_performance_mobile_tiles.parquet\"\n",
    "# a local copy of data is stored here\n",
    "local_path = Path(\"data/internet-speeds.parquet\")\n",
    "if local_path.exists():\n",
    "    net_speed = gpd.read_parquet(local_path)\n",
    "else:\n",
    "    columns = [\"avg_d_kbps\", \"tile\"]\n",
    "    df = pd.read_parquet(url, columns=columns)\n",
    "\n",
    "    tile_geometries = shapely.from_wkt(df[\"tile\"])\n",
    "    tile_centroids = shapely.centroid(tile_geometries)\n",
    "    net_speed = gpd.GeoDataFrame(df[[\"avg_d_kbps\"]], geometry=tile_centroids, crs='EPSG:4326')\n",
    "    net_speed.to_parquet(local_path)\n",
    "net_speed.head()\n",
    "print(f\"There are {len(net_speed)} records in our dataset\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7a3a1fc0-8bd1-4695-8c54-6fdc8583c081",
   "metadata": {},
   "source": [
    "# Creat a map instance\n",
    "layer = ScatterplotLayer.from_geopandas(net_speed)\n",
    "m = Map(layer)\n",
    "m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40dbfbd6-b596-4b31-b23d-72b120a3c6f2",
   "metadata": {},
   "source": [
    "layer.get_fill_color = [0, 0, 200, 200]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c327b18-fa0f-4cd5-8e8e-9239985d4827",
   "metadata": {},
   "source": [
    "# Define the minimum and maximum boundary for normalization\n",
    "min_bound = 5000\n",
    "max_bound = 50000\n",
    "\n",
    "# Extract the average download speeds from the GeoDataFrame column 'avg_d_kbps'\n",
    "download_speed = net_speed['avg_d_kbps']\n",
    "\n",
    "# Normalize the download speeds to a range between 0 and 1 based on defined min and max bounds\n",
    "normalized_download_speed = (download_speed - min_bound) / (max_bound - min_bound)\n",
    "\n",
    "# Access the BrBG_10 colormap from Matplotlib\n",
    "BrBG_10.mpl_colormap\n",
    "\n",
    "# Set the fill color of the layer using the normalized download speed and the BrBG_10 colormap with an opacity of 70%\n",
    "layer.get_fill_color = apply_continuous_cmap(normalized_download_speed, BrBG_10, alpha=0.7)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cb03846-d8ec-4208-810b-a257290683ed",
   "metadata": {},
   "source": [
    "# Set the radius of each point in the layer based on the normalized download speed, scaled by 200\n",
    "layer.get_radius = normalized_download_speed * 200\n",
    "\n",
    "# Specify that the radius values are in meters\n",
    "layer.radius_units = \"meters\"\n",
    "\n",
    "# Set the minimum radius size to 0.5 pixels to ensure visibility at high zoom levels\n",
    "layer.radius_min_pixels = 0.5"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fab24aa0-4cab-4af5-b71a-1cdf905e1a78",
   "metadata": {},
   "source": [
    "#To export as html\n",
    "\n",
    "#import ipywidgets as widgets\n",
    "#map_widget = widgets.HTML(value=m.to_html('map.html'), placeholder='', description='')\n",
    "#display(map_widget)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
