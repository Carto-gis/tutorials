{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1a1642-4f20-44c1-9a44-35a1b9858334",
   "metadata": {},
   "source": [
    "# Big data and cartography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8baf37e-df20-48e4-b9c5-ada89ce81560",
   "metadata": {},
   "source": [
    "In this tutorial, we are going to use Python to explore and visualize some big geospatial data. We are going to work with the following datasets:\n",
    "\n",
    "1. Point dataset of [Flickr](https://www.flickr.com/) posts in Finland 2019â€“2023 acquired through the platform's API. The posts have been stripped of all identifiable information and the exact locations obfuscated using the Laplace noise algorithm implemented in [GeoPriv QGIS plugin](https://diuke.github.io/GeoPrivPlugin/). Only attribute information remaining are pseudo-ids and month-year timestamps. (161544 datapoints)\n",
    "2. Origin-destination line dataset of student mobilities to Germany. \n",
    "3. Internet speeds in the first quarter of 2023 fetched from [Ookla Speedtest](https://www.speedtest.net).\n",
    "\n",
    "In addition to the Python libraries we have already worked with, we are going to get familiar with some additional python libraries that can be handy for handling and visulization of big geospatial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293cbd97-358b-4c38-8ea1-3fce222fce90",
   "metadata": {},
   "source": [
    "## Flickr data from Finland\n",
    "\n",
    "As always, let's start by exploring our data a little bit. Let's load our data, make a quick plot, count the number of points, and print the head of our data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e8c1d3-c637-4443-a706-2cdfddd4c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the data\n",
    "file_path = 'data/flickr_finland_2019_2023_obfuscated.gpkg'\n",
    "flickr_data = gpd.read_file(file_path)\n",
    "\n",
    "flickr_data.plot(markersize=2, edgecolor='none')\n",
    "print(flickr_data.head())\n",
    "print(f\"\\nThere are {len(flickr_data)} points in our dataset\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c9c4b0-deee-41a4-963b-2aaac9f3ae22",
   "metadata": {},
   "source": [
    "As we can see, our data is from Finland and we have 161544 points altogether representing the location of the photos. In our data, we have a column named `month_year` which as name suggests, containes the month and year when the photo was taken. Let's explore the temporal distribution of our data quickly using a time series plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac1cbd-07ad-46d4-9d52-2c1ebbad008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'month_year' to datetime format for better handling\n",
    "flickr_data['month_year'] = pd.to_datetime(flickr_data['month_year'])\n",
    "\n",
    "# Group by 'month_year' and count occurrences\n",
    "time_series = flickr_data.groupby('month_year').size()\n",
    "\n",
    "# Plotting the time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_series, marker='o', linestyle='-', color='b')\n",
    "plt.title('Number of Photos per Month (2019-2023)')\n",
    "plt.xlabel('Month and Year')\n",
    "plt.ylabel('Number of Photos')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()  # Adjusts plot to ensure everything fits without overlap\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f975956-39bb-4392-bb9c-2338c0045d3b",
   "metadata": {},
   "source": [
    "### Hexbin map\n",
    "\n",
    "Hexagons are increasingly favored in the visualization of large geospatial datasets for several reasons:\n",
    "\n",
    "1. **Efficient Coverage and Less Wastage:** Hexagons cover a surface area more efficiently than squares or triangles. When visualizing large areas, hexagonal tiling reduces gaps and overlaps, ensuring more uniform data representation. \n",
    "2. **Uniformity of Data Representation:** Each hexagon has six sides of equal length, which results in a more uniform distance between the center of each hexagon and any point on its boundary. This uniformity ensures that each data point within a hexagon is equally representative of its center. In contrast, square grids have varying distances from the center to the middle of the edges versus the corners, potentially introducing bias in how data is represented spatially.\n",
    "3. **Better Nearest Neighbor Analysis:** Hexagons have a unique advantage due to their six equidistant neighbors, which is beneficial for nearest neighbor analysis. This property ensures that each cell interacts more symmetrically with its surroundings, providing a more natural flow of data and smoother transitions across the grid. Squares, on the other hand, connect to their neighbors at four sides and interact less directly with their diagonal neighbors, which can complicate analyses involving spatial relationships.\n",
    "4. **Visual Appeal and Reduced Visual Errors:** Hexagonal grids tend to be more visually appealing and easier for the human eye to follow. This can enhance the overall readability and interpretation of maps. Furthermore, the equidistant properties of hexagons can reduce visual distortions that often occur with square grids, where the clustering of data points might appear more intense at the corners compared to the center.\n",
    "5. **Efficient Computation:** Despite a seemingly complex shape, algorithms for processing hexagonal grids are often more straightforward and computationally efficient for spatial data analyses than those for squares. This is due to the consistent distances and connectivity, which simplify the computation of spatial relationships and aggregations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6ade4-72c8-40fa-8f51-c0e813c689a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h3\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import mapclassify\n",
    "from shapely import Polygon\n",
    "\n",
    "# for zoomin to Helsinki area, if needed\n",
    "helsinki_bounds = {\n",
    "    \"min_lon\": 24.50,\n",
    "    \"max_lon\": 25.50,\n",
    "    \"min_lat\": 60.00,\n",
    "    \"max_lat\": 60.50\n",
    "}\n",
    "\n",
    "# Function to assign hexagon using H3\n",
    "def assign_hexagon(row, resolution=8):\n",
    "    return h3.latlng_to_cell(row.geometry.y, row.geometry.x, resolution)\n",
    "\n",
    "# Apply function to data\n",
    "flickr_data['hex_id'] = flickr_data.apply(assign_hexagon, axis=1)\n",
    "\n",
    "# Aggregate data within each hexagon\n",
    "hex_counts = flickr_data['hex_id'].value_counts().reset_index()\n",
    "hex_counts.columns = ['hex_id', 'count']\n",
    "\n",
    "# Generate hexagon geometries\n",
    "# there has been a change in the h3 library, and newer versions return boundary coordinates in (lon, lat) order instead of (lat, lon). This was done to align with GeoJSON standards.\n",
    "hex_counts['geometry'] = hex_counts['hex_id'].apply(\n",
    "    lambda x: Polygon([(lon, lat) for lat, lon in h3.cell_to_boundary(x)])\n",
    ")\n",
    "\n",
    "hex_gdf = gpd.GeoDataFrame(hex_counts, geometry='geometry')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Set the bounds for the plot to zoom into Helsinki. if desired \n",
    "#ax.set_xlim(helsinki_bounds[\"min_lon\"], helsinki_bounds[\"max_lon\"])\n",
    "#ax.set_ylim(helsinki_bounds[\"min_lat\"], helsinki_bounds[\"max_lat\"])\n",
    "\n",
    "hex_gdf.plot(ax=ax, column=\"count\",scheme=\"Natural_Breaks\", k=5, cmap=\"RdYlBu\", legend=True, legend_kwds={'loc': 'lower right'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a0cd5-bebc-4bc2-b5a4-c0544d8b571e",
   "metadata": {},
   "source": [
    "Here is a breakdown of what we are doing in the block of code above: \n",
    "\n",
    "*Define a Function to Assign Hexagons:*\n",
    "```python\n",
    "def assign_hexagon(row, resolution=8):\n",
    "    return h3.latlng_to_cell(row.geometry.y, row.geometry.x, resolution)\n",
    "```\n",
    "- Purpose: This function converts geographic coordinates into H3 indices. Each H3 index represents a hexagonal cell on the globe at a specified resolution.\n",
    "- Parameters:\n",
    "  - `row`: A row of a DataFrame, expected to have `geometry` attributes (`y` for latitude and `x` for longitude).\n",
    "  - `resolution`: The granularity of the hexagonal grid. Higher values create smaller hexagons.\n",
    "- Functionality: The function `h3.latlng_to_cell()` takes latitude (`row.geometry.y`), longitude (`row.geometry.x`), and `resolution` to generate a unique H3 index (hexagon ID) for that location.\n",
    "\n",
    "*Apply Function to Data:*\n",
    "```python\n",
    "flickr_data['hex_id'] = flickr_data.apply(assign_hexagon, axis=1)\n",
    "```\n",
    "- Purpose: To assign an H3 hexagon ID to each record in the `flickr_data` GeoDataFrame.\n",
    "- Method: `DataFrame.apply()` applies the `assign_hexagon` function along the DataFrame's rows (`axis=1`).\n",
    "\n",
    "*Aggregate Data Within Each Hexagon:*\n",
    "```python\n",
    "hex_counts = flickr_data['hex_id'].value_counts().reset_index()\n",
    "hex_counts.columns = ['hex_id', 'count']\n",
    "```\n",
    "- Purpose: To count occurrences of each unique H3 index in `flickr_data`.\n",
    "- Functionality:\n",
    "  - `value_counts()`: This function counts how many times each unique value appears in the `hex_id` column.\n",
    "  - `reset_index()`: Converts the Series returned by `value_counts()` into a DataFrame.\n",
    "  - Renaming Columns: Columns are named to `hex_id` and `count` for clarity.\n",
    "\n",
    "*Generate Hexagon Geometries:*\n",
    "```python\n",
    "hex_counts['geometry'] = hex_counts['hex_id'].apply(\n",
    "    lambda x: Polygon([(lon, lat) for lat, lon in h3.cell_to_boundary(x)])\n",
    ")\n",
    "```\n",
    "- Purpose: To convert each H3 index back into a polygon that represents the hexagonal cell on a map.\n",
    "- **Note** that there has been a change in the h3 library (now called h3-py), and newer versions return boundary coordinates in (lon, lat) order instead of (lat, lon). This was done to align with GeoJSON standards, but it breaks older code that assumes (lat, lon).\n",
    "\n",
    "*Convert Hexagon Geometries to GeoDataFrame:*\n",
    "```python\n",
    "def hex_to_polygon(hex):\n",
    "    return Polygon([(lon, lat) for lon, lat in hex])\n",
    "\n",
    "hex_counts['geometry'] = hex_counts['geometry'].apply(hex_to_polygon)\n",
    "hex_gdf = gpd.GeoDataFrame(hex_counts, geometry='geometry')\n",
    "```\n",
    "- Purpose: To transform the list of hexagon vertices into `shapely.Polygon` objects suitable for spatial operations and visualization.\n",
    "- Functionality:\n",
    "  - `hex_to_polygon`: A function that converts a list of (longitude, latitude) tuples into a `Polygon`.\n",
    "  - Applying the conversion function to each hexagonâ€™s vertices.\n",
    "  - Creating a `GeoDataFrame`\n",
    "\n",
    "\n",
    "We will learn how to make interactive plots later in this course. But let's have a quick exploration of our data using plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ff128-acf4-4d07-8426-6344a841644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import mapclassify as mc\n",
    "\n",
    "classifier = mc.NaturalBreaks(hex_gdf['count'], k=10)\n",
    "hex_gdf['count_class'] = classifier.yb  # Assign class labels to the hex_gdf\n",
    "\n",
    "# Plot with Plotly\n",
    "fig = px.choropleth_map(data_frame=hex_gdf, geojson=hex_gdf.__geo_interface__,\n",
    "                           locations=\"hex_id\", color='count_class',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           featureidkey=\"properties.hex_id\",\n",
    "                           map_style=\"carto-positron\",\n",
    "                           zoom=5, center={\"lat\": 64.5, \"lon\": 26.0},\n",
    "                           opacity=0.5, labels={'count':'Data Points'},\n",
    "                           hover_data={'count': True}\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef7966-ebb9-4e0f-b2bd-f67fd2210df8",
   "metadata": {},
   "source": [
    "## Student mobility to Germany\n",
    "\n",
    "We'll work with data that describes [student mobilities in the European Union's Erasmus](https://data.europa.eu/data/datasets/erasmus-mobility-statistics-2014-2020?locale=en) exhange program at the level of the statistical NUTS2 regions. The full dataset has been processed by Tuomas VÃ¤isÃ¤nen and Oula InkerÃ¶inen as part of [Mobi-Twin](https://www.helsinki.fi/en/researchgroups/digital-geography-lab/projects/mobi-twin) project at the [Digital Geography Lab](https://www.helsinki.fi/en/researchgroups/digital-geography-lab/), University of Helsinki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bf2a5-1a63-4330-a387-821679e2f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "# Load the geopackage file for student mobility\n",
    "mobility_file_path = 'data/2018_student_mobility_NUTS2_germany.gpkg'\n",
    "student_mobility_data = gpd.read_file(mobility_file_path)\n",
    "\n",
    "# Assume each code's first two letters are the country code\n",
    "student_mobility_data['origin_country'] = student_mobility_data['ORIGIN'].str[:2]\n",
    "student_mobility_data['destination_country'] = student_mobility_data['DESTINATION'].str[:2]\n",
    "\n",
    "# Filtering only mobilities that end in Germany ('DE')\n",
    "student_mobility_data_filtered = student_mobility_data[student_mobility_data['destination_country'] == 'DE']\n",
    "\n",
    "# Aggregating by origin and destination country to count the number of mobilities\n",
    "country_mobility_counts = student_mobility_data_filtered.groupby(['origin_country', 'destination_country']).size().reset_index(name='counts')\n",
    "\n",
    "# Merging this data back to get a single representative geometry for each pair\n",
    "aggregated_data = student_mobility_data_filtered.drop_duplicates(subset=['origin_country', 'destination_country'])\n",
    "aggregated_data = aggregated_data.merge(country_mobility_counts, on=['origin_country', 'destination_country'])\n",
    "\n",
    "# Load Europe boundaries shapefile\n",
    "europe_boundaries_path = 'data/europe_borders.zip'\n",
    "europe_boundaries = gpd.read_file(europe_boundaries_path)\n",
    "\n",
    "# Ensure both datasets use the same CRS\n",
    "aggregated_data = aggregated_data.to_crs(europe_boundaries.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5eb3be-04ae-4b5a-abd2-be6485092222",
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_boundaries['centroid'] = europe_boundaries.geometry.centroid\n",
    "\n",
    "# Function to find the centroid of the country containing the point\n",
    "def find_country_centroid(point, country_gdf):\n",
    "    for idx, country in country_gdf.iterrows():\n",
    "        if country.geometry.contains(point):\n",
    "            return country.centroid\n",
    "    return None\n",
    "\n",
    "# Relocate each endpoint of the lines\n",
    "def relocate_line(line, country_gdf):\n",
    "    start_point = line.coords[0]\n",
    "    end_point = line.coords[-1]\n",
    "    start_centroid = find_country_centroid(Point(start_point), country_gdf)\n",
    "    end_centroid = find_country_centroid(Point(end_point), country_gdf)\n",
    "    if start_centroid is not None and end_centroid is not None:\n",
    "        return LineString([start_centroid, end_centroid])\n",
    "    return line  # Return the original line if no containing country found\n",
    "\n",
    "# Apply the relocation to all lines\n",
    "aggregated_data['geometry'] = aggregated_data['geometry'].apply(relocate_line, country_gdf=europe_boundaries)\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c2f555-3b8d-4afe-aec7-4598fe340499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "europe_boundaries = europe_boundaries.to_crs(epsg=3034)\n",
    "aggregated_data = aggregated_data.to_crs(epsg=3034)\n",
    "\n",
    "# Optional: Normalize counts for color mapping\n",
    "aggregated_data['normalized_counts'] = aggregated_data['counts'] / aggregated_data['counts'].max()\n",
    "\n",
    "# Choose a colorblind-friendly colormap (e.g., 'viridis', 'cividis', 'plasma', or custom blue-orange)\n",
    "cmap = matplotlib.colormaps.get_cmap('copper')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "europe_boundaries.boundary.plot(ax=ax, edgecolor='gray', linewidth=0.3, alpha=0.4)\n",
    "\n",
    "# Plot mobility lines with color mapping based on counts\n",
    "for _, row in aggregated_data.iterrows():\n",
    "    linewidth = row['normalized_counts'] * 5  # Scale line width\n",
    "    color = cmap(row['normalized_counts'])    # Get color from colormap\n",
    "    line = gpd.GeoSeries(row['geometry'])\n",
    "    line.plot(ax=ax, linewidth=linewidth, color=color, alpha=0.8)\n",
    "\n",
    "# Add a legend\n",
    "# Create custom lines for the legend\n",
    "\n",
    "legend_lines = [\n",
    "    mlines.Line2D([], [], color=cmap(0.1), linewidth=1, label='Low'),\n",
    "    mlines.Line2D([], [], color=cmap(0.5), linewidth=2.5, label='Medium'),\n",
    "    mlines.Line2D([], [], color=cmap(0.9), linewidth=5, label='High')\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_lines, title='Mobility Volume')\n",
    "ax.set_title('Aggregated Student Mobility to Germany by Country')\n",
    "ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1404c0-1069-4ab6-8fcd-77b05b865e93",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 6px solid #4CAF50; background-color: #f9f9f9; padding: 16px; margin: 20px 0; font-family: Arial, sans-serif; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
    "  <h3 style=\"color: #4CAF50; margin-top: 0;\">ðŸ“¢ Note</h3>\n",
    "  <p>For enhanced visualization of complex flows, like what we just created, consider exploring other specialized tools such as the \n",
    "     <a href=\"https://zenodo.org/records/14532548\" target=\"_blank\" style=\"color: #2196F3; text-decoration: none; font-weight: bold;\">\n",
    "     Edge-bundling tool for regional mobility flow data</a>.\n",
    "  </p>\n",
    "  <p>Developed by the <strong>Digital Geography Lab</strong> at the University of Helsinki, this Python-based tool uses \n",
    "     <em>edge-path bundling techniques</em> to significantly reduce visual clutter in flow maps.\n",
    "  </p>\n",
    "  <p>It processes input CSV files containing centroid coordinates and flow data, outputting bundled geometries in GeoPackage format. \n",
    "     This tool is part of the <strong>Mobi-Twin research project</strong>, which focuses on the twin transition and changing patterns of spatial mobility in Europe.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e163310-39af-4727-8ace-8cf5be28653c",
   "metadata": {},
   "source": [
    "## Mapping Speedtest data with lonboard\n",
    "\n",
    "The dataset we are going to use now is from Ookla, known for its [Speedtest](https://www.speedtest.net/) application, which provides data on internet connection speeds across different regions and network types. This specific dataset that we will use here, contains mobile internet performance for the first quarter of 2023. It is stored in the Parquet format, which is highly efficient for handling large datasets as it supports advanced data compression and encoding schemes, making it suitable for big data applications.\n",
    "\n",
    "While vsiualizing our data, we will explore the use of the [`ionboard` library](https://developmentseed.org/lonboard/latest/). This is a new powerful library, designed for efficient handling and analysis of geospatial data. This library is particularly optimized for performance and scalability, which makes it a good choice for processing big geospatial data. \n",
    "\n",
    "\n",
    "Let's start by importing the libraries we need and fetch our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f2046-109a-4187-8bdf-5b9b5c3ab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lonboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba79b81-3a27-4f26-a80f-5ae461953e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb92e2-07a3-41d5-b2e5-06f349795c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from palettable.colorbrewer.diverging import BrBG_10\n",
    "from lonboard import Map, ScatterplotLayer\n",
    "from lonboard.colormap import apply_continuous_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6bb953-471e-4cb1-beff-61cf4dc98872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching data\n",
    "url = \"https://ookla-open-data.s3.us-west-2.amazonaws.com/parquet/performance/type=mobile/year=2023/quarter=1/2023-01-01_performance_mobile_tiles.parquet\"\n",
    "# a local copy of data is stored here\n",
    "local_path = Path(\"data/internet-speeds.parquet\")\n",
    "if local_path.exists():\n",
    "    net_speed = gpd.read_parquet(local_path)\n",
    "else:\n",
    "    columns = [\"avg_d_kbps\", \"tile\"]\n",
    "    df = pd.read_parquet(url, columns=columns)\n",
    "\n",
    "    tile_geometries = shapely.from_wkt(df[\"tile\"])\n",
    "    tile_centroids = shapely.centroid(tile_geometries)\n",
    "    net_speed = gpd.GeoDataFrame(df[[\"avg_d_kbps\"]], geometry=tile_centroids, crs='EPSG:4326')\n",
    "    net_speed.to_parquet(local_path)\n",
    "net_speed.head()\n",
    "print(f\"There are {len(net_speed)} records in our dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3a1fc0-8bd1-4695-8c54-6fdc8583c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat a map instance\n",
    "layer = ScatterplotLayer.from_geopandas(net_speed)\n",
    "m = Map(layer)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dbfbd6-b596-4b31-b23d-72b120a3c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.get_fill_color = [0, 0, 200, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c327b18-fa0f-4cd5-8e8e-9239985d4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the minimum and maximum boundary for normalization\n",
    "min_bound = 5000\n",
    "max_bound = 50000\n",
    "\n",
    "# Extract the average download speeds from the GeoDataFrame column 'avg_d_kbps'\n",
    "download_speed = net_speed['avg_d_kbps']\n",
    "\n",
    "# Normalize the download speeds to a range between 0 and 1 based on defined min and max bounds\n",
    "normalized_download_speed = (download_speed - min_bound) / (max_bound - min_bound)\n",
    "\n",
    "# Access the BrBG_10 colormap from Matplotlib\n",
    "BrBG_10.mpl_colormap\n",
    "\n",
    "# Set the fill color of the layer using the normalized download speed and the BrBG_10 colormap with an opacity of 70%\n",
    "layer.get_fill_color = apply_continuous_cmap(normalized_download_speed, BrBG_10, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb03846-d8ec-4208-810b-a257290683ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the radius of each point in the layer based on the normalized download speed, scaled by 200\n",
    "layer.get_radius = normalized_download_speed * 200\n",
    "\n",
    "# Specify that the radius values are in meters\n",
    "layer.radius_units = \"meters\"\n",
    "\n",
    "# Set the minimum radius size to 0.5 pixels to ensure visibility at high zoom levels\n",
    "layer.radius_min_pixels = 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
